# sensim

attribute in input variable
# input_ids = inputs.get("input_ids")
# attention_mask = inputs.get("attention_mask", attention_mask)
# langs = inputs.get("langs", langs)
# token_type_ids = inputs.get("token_type_ids", token_type_ids)
# position_ids = inputs.get("position_ids", position_ids)
# lengths = inputs.get("lengths", lengths)
# cache = inputs.get("cache", cache)
# head_mask = inputs.get("head_mask", head_mask)
# inputs_embeds = inputs.get("inputs_embeds", inputs_embeds)

XLM model configuration
# vocab_size=30145,
# emb_dim=2048,
# n_layers=12,
# n_heads=16,
# dropout=0.1,
# attention_dropout=0.1,
# gelu_activation=True,
# sinusoidal_embeddings=False,
# causal=False,
# asm=False,
# n_langs=1,
# use_lang_emb=True,
# max_position_embeddings=512,
# embed_init_std=2048 ** -0.5,
# layer_norm_eps=1e-12,
# init_std=0.02,
# bos_index=0,
# eos_index=1,
# pad_index=2,
# unk_index=3,
# mask_index=5,
# is_encoder=True,
# summary_type='first',
# summary_use_proj=True,
# summary_activation=None,
# summary_proj_to_labels=True,
# summary_first_dropout=0.1,
# start_n_top=5,
# end_n_top=5,
###